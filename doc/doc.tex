\documentclass{extarticle}
\usepackage{etex}
%\usepackage[7pt]{moresize}
%\fontfamily{<familyname>}\selectfont
\renewcommand{\sfdefault}{phv}
\usepackage{anyfontsize}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{standalone}
\usepackage{verbatim}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{multicol}
\usepackage{indentfirst}
\usepackage{pstricks,pstricks-add,pst-math,pst-xkey}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{float}
\usepackage{filecontents}
\usepackage{flexisym} %% prime symbol
%Lettres francaises
\usepackage[cyr]{aeguill}
\usepackage[francais]{babel}
\usepackage[T1]{fontenc}    %% permet d'utiliser les caractères accentués
\usepackage[utf8]{inputenc} %% permet d'utiliser les caractères accentué
\usepackage{gensymb}
%Needed for problems with bibtex
\usepackage{hyperref}
\usepackage{harvard}
\usepackage{url}

%PGF plot
\usepackage{pgfplots, pgfplotstable}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{statistics}
\pgfplotscreateplotcyclelist{mycolorlist}{blue,red,brown,teal,violet,cyan,green,scarletred1,black}
\pgfplotscreateplotcyclelist{mylegend}{Ly-a,Target,LRG,ELG,Fake,Fake,SS,SF}
\usepackage{xifthen}

\usepgfplotslibrary{external}
\usetikzlibrary{pgfplots.external} 
\tikzexternalize[prefix=figs/graph/]% activate with a name prefix
%Debug box warnings
\showboxdepth=\maxdimen
\showboxbreadth=\maxdimen
%%%% debut macro %%%%
\newenvironment{changemargin}[2]{\begin{list}{}{%
\setlength{\topsep}{0pt}%
\setlength{\leftmargin}{0pt}%
\setlength{\rightmargin}{0pt}%
\setlength{\listparindent}{\parindent}%
\setlength{\itemindent}{\parindent}%
\setlength{\parsep}{0pt plus 1pt}%
\addtolength{\leftmargin}{#1}%
\addtolength{\rightmargin}{#2}%
}\item }{\end{list}}

%\begin{changemargin}{0cm}{0cm}
%\end{changemargin}

%%%% fin macro %%%%

%Bilio
\usepackage[nottoc,notlof,notlot]{tocbibind}
%Dash lines
\newcommand\Algphasee[1]{%
\Statex\hspace*{-\algorithmicindent}\textbf{#1}%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\linewidth}{0.4pt}%
}

\newcommand\Algphase[1]{%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\linewidth}{0.4pt}%
\Statex\hspace*{-\algorithmicindent}\textbf{#1}%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\linewidth}{0.4pt}%
}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

\def\begeq{\begin{equation}}
\def\endeq{\end{equation}}
\def\begeqar{\begin{eqnarray}}
\def\endeqar{\end{eqnarray}}
\def\Rmm{R_{\rm mm}}
\def\Rdeg{R_{\rm deg}}
\def\micron{\mu{\rm m}}
\def\lya{Ly-$\alpha$\ }
\def\sqd{$deg^{2}$}
\def\psqd{$obj \cdot deg^{-2}$}

\usepackage{geometry}
\geometry{hscale=0.85,vscale=0.85,centering}
\title{Description of fiber assignment code for Mocks in DESI experiment}
%\author{Robert Cahn and Louis Garrigue}
\author[1]{Robert N. Cahn\thanks{rncahn@lbl.gov}}
\author[2]{Louis Garrigue\thanks{louis.garrigue@ens.fr}}
\affil[1]{Department of Cosmological Physics, LBNL, Berkeley}
\affil[2]{Departement de physique, Ecole normale superieure, Paris}
\date{\today}
%--------------------------------------------------------------------------------------------
\begin{document}
\begin{titlepage}
\maketitle
\begin{center}
  \includegraphics[width = 100mm]{figs/black.jpg}

  \includegraphics[width = 20mm]{figs/logolbnl.png} \hfill
  \includegraphics[width = 20mm]{figs/logodesi.jpg} \hfill
  \includegraphics[width = 20mm]{figs/logoens.png} 
\end{center}
\end{titlepage}


\begin{comment}

Dark energy study began in 1998, when two independent teams, led by Adam Riess and Saul Perlmutter, found out, using quasars measurements, that the universe was still expanding. The then standard cosmological model, which components were baryonic and dark matter ruled by general relativity, had to be changed. The $\Lambda$-CDM model now include dark energy, a component explaining the fact that gravity is weaker than expected, and compatible with the exponentially accelerating rate of the universe. It is still today the standard model. One of the main goals of cosmology is then to understand how the universe has been evolving until today, from the oldest times we can reach thanks to the Planck satellite which scrutinizes the Cosmic Microwave Background, picture of the sky 378,000 after the big bang. This study needs to understand the nature of dark energy and has to get information on the distribution of matter over time. This thus involves to scrutinize millions of galaxies and quasars, their spectrum and redshift carrying a lot of data. Such a work needs huge surveys, like BOSS, BigBOSS, and now DESI, heir of the previous ones. DESI will then significantly improve the knowledge on caracteristic features, espacially the power spectrum (Fourier transform of the correlation function of the matter density) which permits to understand baryon acoustic oscillations (BAO) which went through the early universe and which signature has always been remaining. One of the pivotal points of the DESI experiment is the assignment of its 5000 fibers on 10666 different tiles, watching galaxies in a mock catalog of 70M ones. Indeed, the efficiency of DESI will directly depend on multiple features of this assignment algorithm : number of unassigned fibers, number of times a QSO \lya will be observed (5 times goal), etc. Here is a brief presentation of the standard cosmological model and the DESI experiment. We then describe the principles used, rules, C++ code organisation and results of the assignment we created, run on the Edison NERSC Supercomputer.


\tableofcontents

	\section{Cosmology today}
\subsection{Recent breakthroughts}


	\section{Cosmology today}

	\subsection{Dark matter}
	Dark matter is invisible (doesn't react with electromagnetic radiations), and has been discovered when physicists noticed that there must be additional matter in clusters of matter. It accounts for 27\% of the totality of matter and implies that the standard model of particle physics must be incomplete. Through gravitational interaction with baryonic matter, they influence the large scale structure formation, but also small-scale clustering of galaxies lying in host dark matter halos.


	\subsection{\Lambda-CDM model}
	This is the currently standard model. It assumes that the universe is ruled by regular general relativity, and that the existence of dark matter and of the Einsein's cosmological constant \Lambda.
	It respects the cosmological principle : our observational location in the Universe is not unusual or special ; on a large-enough scale (much greater than the typical size of galactic clusters), the Universe looks the same in all directions (isotropy) and from every location (homogeneity). Then comes the well-known FLRW metric : 
	
\begin{equation}\label{metric}
	g_{\mu\nu}dx^{\mu}dx^{\nu} = -c^{2}dt^{2}+a(t)^{2}(\frac{dr^{2}}{1-kr^{2}}+r^{2}d\Omega)
\end{equation}



There is a growing consensus among cosmologists that the total density of matter is equal to the critical density, so that the universe is spatially flat ($k = 0$). Approximately 24\% of this is in the form of a low pressure matter, most of which is thought to be “non-baryonic” dark matter, while the remaining 71\% is thought to be in the form of a negative pressure “dark energy”, like the cosmological constant. If this is true, then dark energy is the major driving force behind the fate of the universe and it will expand forever exponentially.
Dynamical equation used is Einstein's field equation :
\begin{equation}\label{eq1}
R_{\mu \nu} - {1 \over 2}g_{\mu \nu}R + g_{\mu \nu} \Lambda = {8 \pi G \over c^4} T_{\mu \nu}
\end{equation}


We apply it to the FLRW metric, on a perfect fluid of energy density $\rho$ and pressure $p$ to get Friedmann equations :
\begin{equation}\label{eq2}
	\bigg(\frac{\dot{a(t)}}{a(t)}\bigg)^{2} + \frac{c^{2}k}{a^{2}} = \frac{8 \pi G}{3}\rho + \frac{c^{2}\Lambda}{3}
\end{equation}

\begin{equation}\label{eq3}
\frac{\ddot{a(t)}}{a(t)} = - \frac{4\pi G}{3}(\rho + 3\frac{p}{c^{2}}) + \frac{c^{2}\Lambda}{3}
\end{equation}


Since it's a perfect fluid, we also use $p = w\rho c^{2}$. One of the most interesting thing to compute from them is the scale factor $a(t)$. Using those Friedmann equations, one easily gets :
$\frac{\dot{\rho}}{\rho} = -3(1+w)\frac{\dot{a}}{a}$, and then, if $w$ is a constant : $\rho \propto a^{-3(1+w)}$.

For baryonic/leptonic matter, $w = 0$. For radiation, $w = 1/3$. The cosmological redshift $z$ is defined by $1+z \equiv 1/a$, the Hubble parameter by $H(z) \equiv \frac{\dot{a}}{a}$ and $H_{0}$ is the Hubble parameter today. 

We define the energy density parameter for matter and radiation $\Omega \equiv \rho_{i}/\rho_{critic}$, where $\rho_{critic} \equiv \frac{3H_{0}^{2}}{8\pi G}$is the critical density of the universe, beyond which it would recollapse due to gravitation. We also define $\Omega_{k} \equiv -c^{2}k/H_{0}^{2}$ and $\Omega_{\Lambda} \equiv c^{2}\Lambda/3H_{0}^{2}$, energy densities associated with curvature and cosmological constant \cite{2013PhR}.

The first Friedmann equation \ref{eq2} can then be written :
\begin{equation}\label{eq4}
\frac{H(z)^{2}}{H_{0}^{2}} = \Omega_{m}(1+z)^{3} + \Omega_{r}(1+z)^{4} + \Omega_{k}^{3} + \Omega_{\Lambda}
\end{equation}

This is the relation between dark energy (through its equation of state $w(z)$) and the expansion (through $H(z)$).

	\subsection{Dark energy}
Saul Perlmutter's investigations showed that $\ddot{a(t)}}$ is actually positive, which means that the expanding rate of the universe is still inscreasing and this was the beginning of dark energy hypothesis. That's why the cosmological constant was reintroduced. An other possibility is that the pressure of "dark energy" could be negative.

	Several other theoretical solutions \cite{2013zfa} are possible. Dark energy is in fact a catch-all term for the origin of cosmic acceleration, and could be either a modification of general relativity, or a new kind of energy.

In this equation, the cosmological constant is equivalent to a fluid with state parameter $w = −1$ and a repulsive gravitational effect, and we call it dark energy as we don’t know anything else about it. Now, let’s imagine that dark energy as a fluid that fills the whole Universe but with a parameter $w$ that can evolve in time, i.e. depending on the scale parameter $a$. We find then that equation is modified by $\Omega_{\Lambda} \leftarrow \Omega_{\Lambda}exp(3 \int_{0}^{z} \frac{1+w(z\textprime)}{1+z\textprime} dz\textprime)$

Here are other several possibilities of modified gravity :
\begin{itemize} 
	\item replace the Ricci scalar $\mathcal{R}$ with a function $\mathcal{R} + f(\mathcal{R})$.
\end{itemize} 
	
	Two possible new energies are the cosmological constant (a constant energy density filling space, and which can be formulated to be equivalent to vacuum energy, with repulsive gravitational effects) and scalar fields such as quintessence or moduli, dynamic quantities whose energy density can vary in time and space. Scalar fields that do change in space can be difficult to distinguish from a cosmological constant because the change may be extremely slow.
	The density of dark energy, $7 \cdot 10^{−27} kg \cdot m^{-3}$ (with mass-energy equivalence), is much lower than the density of ordinary and matter within galaxies. It comes to dominate the mass–energy of the universe because it is uniform in space.

One can consider that 

	\section{DESI}
	The Dark Energy Spectroscopic Instrument (DESI) is a Stage IV experiment that will make the next major advance in dark energy. \cite{2013gra}

	\subsection{Material features}
	The DESI instrument, that will be installed on the Mayall telescope in Arizona, consists of a new wide-field (3.2 deg. linear field of view) corrector plus a multi-object spectrometer with up to 5000 robotically positioned optical fibers, with a resolving power of R = 5000. The fibers feed 10 three-arm spectrographs (500 fibers for each spectrographs, also called petals) producing spectra that cover a wavelength range from 360-980 nm and have resolution of 2000-5500 depending on the wavelength. It will watch at 14,000 \sqd in 10666 tiles from 2018 to 2022, with roughly 7 different tiles each night. Fiber positioners will have a reach of $\sim$6 mm from their central position.

	\subsection{General aspects}
	It will study baryon acoustic oscillations (BAO) and the growth of structure through redshift-space distortions.
	The goals are : i) probe the effects of dark energy on the expansion history using BAO, ii) measure the gravitational growth history through redshift-space distortions, iii) measure the sum of neutrino masses, and iv) study the signatures of primordial inflation. The resulting 3-D galaxy maps at z < 2 and Lyman-alpha forest at z > 2 will make 1\%-level measurements of the distance scale in 35 redshift bins. This will provide unprecedented constraints on cosmological models.

	This survey is the successor of the Stage-III BigBOSS survey and complements imaging surveys such as the Stage-III Dark Energy Survey (DES, currently operating) and the Stage-IV Large Synoptic Survey Telescope (LSST, planned start in the next decade).

	\subsection{Tiles}
	In average, a galaxy is covered 5.24 times (by different fibers), with only 2.6\% of the footprint having a coverage of 3 or less. 
	Figure du tiling. Figure des fibers.
	There are different tiling strategies, depending on what 


\subsection{Targets}
It will seek for targets that trace the evolution of dark energy from redshift 0.2 out to 3.5 using 3 kinds of celestial objects : 4 million luminous red galaxies (LRG), 18 million emission line galaxies (ELG) and 3 million quasars. QSO \lya are the most important targets. Then comes LRG, then ELG. For each petal (of 500 fibers), 10 are dedicated to Standard Stars and 40 to Sky Fibers.
\begin{itemize} 
	\item QSO (quasi stellar object) targets are divided into three categories. There are those at $z < 2.1$ (QSO-I), which are used as tracers only, and as those at $z > 2.1$ which are used for the \lya forest. Finally, there are false ones, objects that are a priori believed QSO but are actually not (known once observed). Quasars are extremely luminous, there spectra is very large and doesn't correspond to a star's one. They are likely central regions of massive galaxies surrounding a supermassive black hole, and their energy release must be caused by mass falling onto the accretion disc of the black hole. They are excellent tracer population at redshifts $1 < z < 2$. QSO \lya with $z > 2.1$ are espacially important because it is the only opportunity for DESI to get information on dark energy at so high redshifts, and get information on the matter distribution between us and the quasar. The Lyman-$\alpha$ forest represents the holes in the spectrum that correspond to an absorption ray of the hydrogen \cite{2003AIPC} (at different redshifts), and by this mean we can compute the matter distribution of hydrogen (consequently of all baryonic matter) along the line of sight.
	\item LRG (luminous red galaxy) are the most luminous and red galaxies in the universe at $z < 1$ \cite{2001cq}.
	\item ELG (emission line galaxy) are galaxies where stars are creating. They have strong emission lines and characteristic OII and OIII doublets at a rest frame 327 nm and 500nm, which enable to precisely get their redshift.
	\item Sky Fibers and Standard Stars are calibration targets, necessary to have a reference during measurements. There have to be 10 Standard Stars and 40 Sky Fibers on each petal
\end{itemize} 

\subsection{Fiber assignment}
Fiber assignment is the correspondance between fibers of each tile to objects. It tells us which fiber is assigned to which galaxy, or unassigned (in regions where there are not enough galaxies). It depends upon fiber locations on the plate, and on the tilling pattern of the sky. One has to make prioritizations because there are several possibilities to target selection for each fiber. 


\section{Rules for fiber assignment}
We know the repartition of matter (by repartition of H matter) by pics.
The original catalog of 50M galaxies has only information on the position and the type of the galaxy, which is LRG, ELG or QSO. Those information were given by a previous survey, only based on colors, the redshift wasn't then computed. But there are different types of each one. QSO can be \lya, target or fake (actually fake ones can be stars for example). A python code generates the file of galaxies, simulating with appropriate probabilities each type, taking into account the fact that there are correlations between positions of galaxies (accordingly with BAO and gravition). When we watch at a QSO and we see that it's a fake or a target, we don't watch at it anymore. But if it's a real one, we try to see it 5 times if it is possible.
The absorption spectrum shows a pic at $\lambda = 1216$ \AA. We can deduced the redshift because we receive a wavelenght $\lambda(1+z)$. As spectrometers can only see wavelenghts in the range 380 nm (under, no enough energy is received) - 980 nm, the minimum redshift we can reach is 2.2.
At first, there was as code written by Martin White then Bob Cahn that did an assignment of tilefibers - galaxies in a global way : naive assignment, improvement (try to reassign some fibers to other ones to assign free ones) and redistribution (do not assign more, but it's to have a better repartition of free fibers on each petal).
We have to reserve 10 fibers by petal to Standard Stars and 40 to Sky Fibers.
We have to avoid unmastered correlations. That's why we don't (at first sight), when we take a new tile, associate 50 free fibers to regions of low QSO density. We would rather assign until there are only 50 free fibers left.
We do the compuation in real time. That means that we do it tile by tile. We can't access the information of all the galaxies because we only have past watched galaxies information. That adds a new constrain. So I rewrote algorithms so that they would compute an optimal assignment for one plate, knowing information on previous one assignments, and knowing information on previously watched galaxies.

\end{comment}


\section{Introduction}
Martin White developed C++ code for fiber assignment over the full 14k \sqd footprint.  We first modified Martin's code to incorporate features from Bob Cahn's python code, which ran on a restricted 480 \sqd... We included the improvement and redistribution steps, which switch fiber assignments to increase the number of galaxies observed. We then adapted the code to compute assignments not globally "knowing all information on galaxies yet to be observed" but plate after plate, as in the real experiment.
The samples are taken from Martin's mocks in stored on NERSC at /project/projectdirs/desi/mocks/preliminary/. From these files we create a single file containing the appropriate mix of ELG, LRG, QSO, SS (Standard Stars) and SF (Sky Fibers) using the python script in git fiberassign/bin/make\_catalog\_starsandsky.py. In the same place there is python code to produce a mixture of galaxies without any correlations, but with the correct $\frac{dn}{dz}$  
  
\section{In a nutshell}
\subsection{Input files}
Found in the NERSC repository (except for Parameters file features.txt).
The produced executable is {\tt assign}. The calling sequence will look like :
{\tt./assign features.txt}. An example of how runing it on NERSC is provided in the {\tt run} script, then call it with qsub run.

\begin{itemize} 
	\item Parameters : all parameters used in the fiber assignment are written in this file. It contains the adress of other input files
	\item Target DB : information, before the study, on all possible targets : (NERSC rep) /projects/projectdirs/desi/mocks/preliminary/objects\_ss\_sf0.rdzipn, created by {\tt make\_catalog}
	\item Obs DB : batabase constructed from the ongoing DESI observations after the data has been processed by the Spectroscopic pipeline. This DB has not been designed yet
	\item Survey tiles : file containing the positions of all the tiles to be observed in /project/projectdirs/desi/software/edison/desimodel/0.3.1/data/footprint/desi-tiles.par
	\item Fiber positions : locations of the positioners in the focal plane in /project/projectdirs/desi/software/edison/desimodel/0.3.1/data/focalplane/fiberpos.txt
\end{itemize} 

To change the location of other input files, one can simply change it in the features file.

\subsection{Output files}
They are produced in a directory defined in features.txt (/project/projectdirs/desi/users/rncahn/fa\_output/ for now), in the format {\tt tile54.fits} for example for the 54th tile. There are therefore 10666 such binary files. They consist in 5000 lines (fibers) with the following columns :

\begin{itemize} 
    \item fiber: [0-4999]
    \item positioner: [0-4999] (not provided yet)
    \item number of available objects
    \item ID of available objects
    \item objtype: ELG, LRG, QSO, SKY, STDSTAR, GAL, OTHER (actually QSOLy-a, or QSOFake etc for now) (-1 if the fiber isn't assigned)
    \item targetid: unique target identifier to get back to target selection info
    \item desi target0: 64 bit mask of targeting info (not yet)
    \item ra: degrees [0-360]
    \item dec: degrees [-90 - +90]
    \item xfocal: mm from center in positioner coordinate system
    \item yfocal: mm from center in positioner coordinate system 
\end{itemize} 

\section{Source files}
Source files are file.h and file.cpp and are in this increasing dependency order :
\begin{itemize} 
	\item misc : a home-made library of structures (and functions on them) needed to manipulate concerning datas, but independent of them. There are pair (of int), List (of int), Table, Cube, and timing, printing, string conversion, error report items.
	\item collision : used to compute collision checkings and build polygons
	\item features : stocks all useful parameters
	\item structs : structures of the manipulated datas and their members
	\item global : main high-level functions and algorithms used in the program to collect information, assign fibers and print statistics. Important ones are described further
	\item main : neat and quickly understandable code that sums up all steps
\end{itemize} 

\section{Parameters}
Here are some features on input galaxies simulated catalog :


\begin{table}[H]\centering
	\begin{tabular}{rcccl} \hline
		Kind&Id&Priority&Nobs&Density (objects/\sqd)\\ \hline
		QSO Ly-$\alpha$ & 0 & 1 & 5 & 50\\
		QSO Tracer & 1 & 1 & 1 & 120\\
		LRG & 2 & 3 & 2 & 300\\
		ELG & 3 & 5 & 1 & 2400\\
		Fake QSO & 4 & 1 & 1 & 90\\
		Fake LRG & 5 & 3 & 1 & 50\\
		Standard Star & 6 & 2 & 1 & 140\\
		Sky Fiber & 7 & 4 & 1 & 1400\\ \hline
	\end{tabular}
	\caption{Characteristics of galaxy samples as set in make\_catalog\_rnc.py}\label{table:characteristics}
\end{table}

A number of parameters are defined at the begining of the file main.
\begin{itemize} 
	\item parameters summed-up in the Table \ref{table:characteristics}
	\item Npass = 5 number of passes
	\item MinUnused = 50 (not used anymore for the moment) minimum number of unused fibers on each petal
	\item MaxSS = 10 ; MaxSF = 40 maximum number of fibers assigned to SS and SF on a petal
	\item PlateRadius = $1.65^{\circ}$ radius of the plate
	\item TotalArea = 15789.0 \sqd total area of the sky considered
	\item invFibArea = 700 inverse of area in \sqd accessible to a fiber (fiber density for a \sqd)
	\item NeighborRad = 14.0 mm maximum distance to consider that two fibers are neighbors
	\item PatrolRad = 6.0 mm maximum distance, on plate coordinates, that allows a fiber to a galaxy
	\item InterPlate = 200 minimal number of plates between two observations of the same galaxy
	\item Randomize = false randomize order of plates in making plans
	\item Pacman = false selects only spectrometers 0, 1, 2, 7, 8, 9 of the pacman

	\item Collision = true when we want to allow collisions, to compute the collision rate for instance
	\item Exact = true whether we want exact collision checking (exact geometry of components) or just circles
	\item AvCollide = 2.9 mm in case of no exact collision checking, limit distance between two galaxies for their positioners to collide
	\item Collide = 2.1 mm minimum distance allowed on plate projection of two assigned galaxies on the same plate (optimizes collision checking)
	\item NoCollide = 7 mm maximum distance between two galaxies for the collision of their corresponding galaxies (optimizes collision checking)
\end{itemize} 


\section{Classes and structures}
Classes and structures are built to be independent of each other, flexible, quickly understandable, logical, and with no redundant information as much as possible.

\begin{table}[H]\begin{center}
	\begin{tabular}{|l|l|p{12cm}|} \hline
		Structure name & Meaning & Description \\ \hline \hline

		PP & Plate Parameters & Carries locations of fiber positions on the plate, spectrometer correspondence, and neighboring fibers information.\\ 

		onplate & Plate coordinates & Used for coordinates in the focal plane in mm. The member {\tt id} is used to give the identity of a galaxy. Onplates is vector of onplate.\\ 

		plate & A plate & Locations in the sky of the tile in terms of a unit vector derived from RA and DEC. Carries also the Id of the tile, its pass, and the available galaxies it is able to reach. Plates is vector of plate, and has all information on tiles.\\ \hline

		galaxy & A galaxy & Information on a galaxy : an {\tt id} that corresponds to Table \ref{table:characteristics}, a position in the sky (in two differents ways), and the available tile-fibers that can observe it. Gals is vector of galaxy and carries all information on galaxies.\\ \hline

		Assignment & An entire assignment & Carries mapping of tile-fibers to galaxies, its inverse, galaxies to tile-fibers, and the cube (3d-matrix) of assigned fibers for a kind, a petal and a plate (useful for fast computations).\\ \hline
	\end{tabular}\end{center}
	\caption{Classes and structures}\label{tab:structures}
\end{table}


\section{Functions}
In algorithms, j stands for a plate, k for a fiber, p for a petal, g for a galaxy.

\subsection{Some functions in structs.cpp}
{\tt plate\_dist} turns radians into mm on the focal plane, i.e. it is the plate scale as a function of angle.

{\tt change\_coords} combines a galaxy and a particular plate to give the coordinates the galaxy will have in the focal plane when observed as with this tile. It's a rotation in angular coordinates. This ought to be rigorously checked.

{\tt find\_collision} returns the fiber number of a fiber that conflicts with tiblefiber (j,k).  Conflict is defined by two observed galaxies being separated by less than {\tt Collide}, currently set to 2.9 mm.

\subsection{Collecting}
{\tt collect\_galaxies\_for\_all} is multithreaded, and for each fiber of each tile, collects reachable galaxies. It uses kdTree and htmTree libraries written by Martin White, because it's absolutely necessary to do computations in a reasonable time with supercomputers.

{\tt collect\_available\_tilefibers} computes, using the previous work, available tile-fibers for each galaxy (inverse map)

\subsection{Useful sub-functions for global functions}

{\tt ok\_assign\_g\_to\_jk\_nobs and ok\_assign\_tot} checks to see if we can assign g to the tile-fiber (j,k), according to assigning rules described further

{\tt find\_best (j,k)} finds the best reachable galaxy for this fiber, according to assignment rules

\begin{algorithm}[H]
	\caption{Find best(j,k)}\label{euclid}
	\begin{algorithmic}[1]
		\State Initialize a fictional galaxy with $ID=-1$, called best
		\For {each galaxy available galaxy (for this fiber) g}
		\If {g is better (according to priority and number of observations)}
		\State $best \gets g$
		\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}

{\tt assign\_fiber(j,k)} tries to assign this fiber using find\_best

{\tt improve\_fiber (j0,n,j,k)} if this fiber is unused, tries first to simply assign it, and if it doesn't work, tries to reassigning some used one (jp,kp) where $j0\le jp \le j0+n$. Before : (jp,kp) - g ; (j,k) \& gp free. After : (j,k) - g \& (jp,kp) - gp. The power of this function lies in the fact that j and jp can correspond to different passes.

\begin{algorithm}[H]
	\caption{Improve\_fiber(j0,n,j,k)}\label{euclid}
	\begin{algorithmic}[1]
		\If {k is not assigned}
		\State try to assign running assign\_fiber(j,k)
		\If {k couldn't be assigned this way}
		\State initialize a set of variables jp, kp, g, gp
		\For {each galaxy g available to (j,k)}
		\If {it's possible to assign g with k}
		\For {each chosen tile-fibers (jp,kp) which chose g (where $j0\le jp \le j0+n$)}
		\State $gp\leftarrow find\_best(jp,kp)$
		\State memorize jp, kp, g, gp if it's a better set (gp is more worthy) than previous one
		\EndFor
		\EndIf
		\EndFor
		\If {$gp \ne -1$}
		\State Unassign $(jp,kp) \longleftrightarrow g$
		\State Assign $(j,k) \longleftrightarrow g$
		\State Assign $(jp,kp) \longleftrightarrow gp$
		\EndIf
		\EndIf
		\EndIf
	\end{algorithmic}
\end{algorithm}


\subsection{Assigning making a plan}
In the simulated catalog, we know all information on objects (if a QSO is a real one for example). During the real study, one won't have access to this information prior to the observation. Thus, in the code, we have to simulate that we have this infotmation only when we have at least once observed the object.

Thus, algorithms do a plan with only information available by the catalog and by previously seen galaxies. A plan consists of an assignment on a set of consecutive tiles (from 0 to 2000 for instance). One can use algorithms on size-one plans if one wants to do an assignment plate after plate for example (which shouldn't be called a plan anymore).

The argument "next" (integer) in following functions means that we treat all next "next" plates (in the right order of tiles) in the plan we make. If it is $-1$, it will deal with all left plates. For example, if next plate in the program is 10 and one launch a function with $next = 100$, this function is going to do his job on plates from 10 to 110.

{\tt simple\_assign} makes a first simple assignment plan : for each fiber assign to the best available galaxy

\begin{algorithm}[H]
	\caption{Simple\_assign(j0,n)}\label{euclid}
	\begin{algorithmic}[1]
		\For {each plate j from j0 to j0+n}
		\For {each fiber k in a random order}
		\State try to assign\_fiber(j,k)
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}

{\tt new\_assign\_fibers} makes a first assignment plan trying to assign first QSO, then ELG and LRG, then trying to assign SF and SS replacing other kinds if there are not enough available fibers. This way, a QSO can't be lost because of a collision. With the current value of Collision, it's not very useful, but could be for bigger values.

\begin{algorithm}[H]
	\caption{New\_assign\_fibers(j0,n)}\label{euclid}
	\begin{algorithmic}[1]
		\For {each plate j from j0 to j0+n}
		\For {each petal p of this plate, in a random order}
		\For {each fiber k of this petal, in a random order}
		\State Try assign\_fiber(j,k) only allowing QSO
		\EndFor
		\For {each fiber k of this petal, in a random order}
		\State Try assign\_fiber(j,k) only allowing ELG and LRG
		\EndFor
		\For {each unassigned fiber k of this petal}
		\State Try assign\_fiber(j,k) to only SS or SF
		\EndFor
		\If {Number of SS $\le$ 10}
		\State Replace ELG (then LRG) to an SS until there are 10 SS
		\EndIf
		\If {Number of SF $\le$ 40}
		\State Replace ELG (then LRG) to an SF until there are 40 SF
		\EndIf
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}

{\tt improve} improves the plan applying improve\_fiber to all of the fibers that concern it

{\tt improve\_from\_kind (kind)} for every concerned petal, tries to assign unassigned fibers to SS or SF, to release an other fiber (formerly assigned to a SS or SF) that would be reassigned to a regular galaxy (kind is SS or SF when we call it). There is also the function {\tt improve\_fiber\_from\_kind} that does the same but on only a fiber.

\begin{algorithm}[H]
	\caption{Improve\_from\_kind (kind,j0,n)}\label{euclid}
	\begin{algorithmic}[1]
		\For {each plate j from j0 to j0+n}
		\For {each petal p of this plate, in a random order}
		\For {each unassigned fiber k}
		\State initialize a set of variables kp, g, gp
		\For {each available galaxies g of k}
		\For {each fiber kp of p assigned to a galaxy of kind}
		\If {no conflict \& g is worthier than previous one}
		\State memorize kp, g, gp
		\EndIf
		\EndFor
		\EndFor
		\If {$g \ne -1$}
		\State Reassign kp to g
		\State Assign k to a galaxy gp of kind kind
		\EndIf
		\EndFor
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}

{\tt update\_plan\_from\_one\_obs} updates the plan formerly made so that if for example on the previous observation we've watched at a fake QSO, it won't be observed anymore in case it was planed to be observed once or several times again. Released tile-fibers will then be tryed to be reassigned with improve\_fiber

\begin{algorithm}[H]
	\caption{Update\_plan\_from\_one\_observation(j0,end\_plan)}\label{euclid}
	\begin{algorithmic}[1]
		\State get the list of galaxies observed by this plate that are discovered fake or target
		\For {each of those galaxies g}
		\State get the list of tile-fibers that are supposed to observed this galaxy further in the plan $j0\le jp \le end$
		\For {each of those tile-fibers (jp,kp)}
		\State Unassign $(jp,kp) \longleftrightarrow g$
		\State run improve\_fiber(j0+1,end-j0,jp,kp)
		\If {jp, kp hasn't been assigned}
		\State run improve\_fiber\_from\_kind(SF,j0+1,end-j0,jp,kp)
		\EndIf
		\If {jp, kp hasn't been assigned}
		\State run improve\_fiber\_from\_kind(SS,j0+1,end-j0,jp,kp)
		\EndIf
		\EndFor
		\EndFor
	\end{algorithmic}
\end{algorithm}


\subsection{Displaying results}
{\tt results\_on\_inputs} displays some statistics on input files (recall input features, print statistics on the number of fibers with 0 galaxies within reach, 1 galaxy within reach etc. out to 19 galaxies within reach ; number of available tile-fibers for a galaxy, ...)

{\tt display\_results} writes some statistics and provides the tex-formatted results to make Table \ref{tab:full14k}

{\tt print\_free\_fibers} print histograms on free fibers towards/regarding petals, for everything, for only SS and for only SF


\section{Rules of assignment}
\begin{itemize}
	\item Of course, a tile-fiber is only assigned once
	\item two observations of a galaxy can't be separated by less than {\tt InterPlate} (200) plates
	\item two observed galaxies can't be separated by less than {\tt Collide} (set by find\_collision)
	\item in last pass, only ELG, SS and SF are considered
	\item there can't be more than 10 fibers assigned to SS on a petal
	\item there can't be more than 40 fibers assigned to SF on a petal
	\item basically, we choose best galaxy from at the same time available, observed less than maxgoal(kind) times and unassigned ones. Among them, we take those with highest priority, and then the one which has been seen the least number of times
\end{itemize}

In nature we only know the basic type (QSO, LRG, ELG, SS, SF) of the galaxies of the catalog. The precise type is only infered by a simulation coded in python, but is generated with randomness. So in the algorithms, we can know the nature of the object only when it has been observed once at least.
There are two way of assignment : either observe tile after tile and foresee the assignment of the tile just before observing it ; otherwise, we can make a plan for a definite number of succeeding tiles. In such a plan, we try to optimize assignment, but of course, we can't know the precise type (fake, ...) so it is possible to foresee observing for example 4 times a fake QSO. We won't do it, so after having planned, when we begin to apply it and do the real observation, if we have just previously observed a fake QSO, we will remove further observations in this plan. We then try to assign the released tile-fibers. In the code, a difficulty was optimization at this point : we could have replanned everything in the plan after an update of information, but we can't because that would take too much time. So we only try to reassign released tile-fibers.

The idea is to get most information possible in the first pass, assigning independently tile after tile, and then, after this first pass, make a plan for everything else, when we have information on most of the QSO and LRG (the only kinds we want to observe several times). The plan will be better that way, because we do less mistakes than if we would do it from the begining.


\section{Algorithm}

The calling sequence is {\tt assign galaxy plate\_centers fiberpos assignment 1 1}. The last of these is the file to which we would write all the final galaxy assignments, but this is generally suppressed since we aren't using it yet.

 {\tt main} proceeds by taking important parameters, features of galaxies, reading in the galaxies to make G, the positions of the fibers on the plate to make pp, the plate centers and positioner locations to make P. 
 
We then collect available galaxies for each fiber for each plate, and compute the inverse map.
 
We next, tile after tile, do the assignment of galaxies. We have tried several strategies : making several plans, use simple assignment, use complexe one, improve several times, in several ways, etc... Here is the one which gives the best results.

We present the strategy that produced the best results, which will be the strategy of reference,
We tried a strategy where we make then apply a plan for the first 2000, then for the following 2000, then for the left, but it's not better.

Simple assign fibers and new assign, even if there structure are quite different, give almost the same results. Simple assign is a little bit quicker so we will use this one.

\begin{algorithm}[H]
	\caption{Assignment of reference in main program}\label{euclid}
	\begin{algorithmic}[1]
		\Algphasee{Phase I - Make a plan for the first 2000 plates}
		\State Run, globally, on the list plates from 0 to 1999 :
		\State Assign fibers
		\State Improve SF
	\end{algorithmic}
	\begin{algorithmic}[1]
		\Algphase{Phase II - Applying the plan}
		\For {each plate j of the plan, in order}
		\State Real observation is here
		\State Update information collected on previous just watched tile
		\State Update\_plan\_from\_one\_observation(j)
		\EndFor
	\end{algorithmic}

	\begin{algorithmic}[1]
		\Algphase{Phase III - Make a plan for all left plates}
		\State Run, globally, on the list plates from 2000 to the end :
		\State Assign fibers
		\State Improve
		\State Improve SF
		\State Improve SS
		\State Redistribute
		\State Improve
		\State Redistribute
		\State Improve
	\end{algorithmic}

	\begin{algorithmic}[1]
		\Algphase{Phase IV - Applying the plan}
	\end{algorithmic}
\end{algorithm}


A standard output display of an execution look like that (for assignment plan and applying):
{\tt \small 

 - Plate 1998 :      9 not as -     0 unas \&    0 replaced

 - Plate 1999 :      8 not as -     0 unas \&    0 replaced

\# ... took : 10.5 s

\# Begin simple assignment :

  37,635,165 assignments on all left next plates

\# ... took : 2 mn 17.3 s

\# Begin improve :

  921,594 more assignments (2.449 \% improvement)

\# ... took : 42.1 s

\# Begin improve SF :

  1,386,726 more assignments (3.597 \% improvement)

\# ... took : 3 mn 13.8 s

\# Begin improve SS :

  194,603 more assignments (0.487 \% improvement)

\# ... took : 6.99 s

\# Begin redistribute TF :

  2,202,483 redistributions of couples of TF

\# ... took : 16.8 s

\# Begin improve :

  167,146 more assignments (0.416 \% improvement)

\# ... took : 23.3 s

\# Begin redistribute TF :

  2,058,702 redistributions of couples of TF

\# ... took : 16.8 s

\# Begin improve :

  76,484 more assignments (0.190 \% improvement)

\# ... took : 22.1 s

\# Begin real time assignment at 11 mn 34.7 s

 - Plate 2000 : SF-imp + 38 (+0.769 \%) SS-imp +  1 (+0.020 \%)    22 not as -  2630 unas \& 1985 replaced

 - Plate 2001 : SF-imp + 37 (+0.750 \%) SS-imp +  1 (+0.020 \%)    29 not as -  3227 unas \& 2619 replaced

 - Plate 2002 : SF-imp + 34 (+0.691 \%) SS-imp +  2 (+0.040 \%)    44 not as -  3296 unas \& 2632 replaced

 - Plate 2003 : SF-imp + 13 (+0.262 \%) SS-imp +  1 (+0.020 \%)    17 not as -  2797 unas \& 2526 replaced
}

\section{Results}
\subsection{Results on the input catalog}
Here are some statistics on the input galaxies catalog :

\begin{figure}[H]\begin{center}\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{avgalhist}
	\begin{tikzpicture}[scale=1.1]\begin{semilogyaxis}[const plot,enlarge x limits=false,xmax=10,xlabel={\# of accessible galaxies},ylabel={},cycle list name=mycolorlist,legend entries={QSO \lya,QSO Tracer,LRG,ELG,Fake QSO,Fake LRG}] 
		\addplot table[x=x,y=0] {figs/avgalhist.dat};
		\addplot table[x=x,y=1] {figs/avgalhist.dat};
		\addplot table[x=x,y=2] {figs/avgalhist.dat};
		\addplot table[x=x,y=3] {figs/avgalhist.dat};
		\addplot table[x=x,y=4] {figs/avgalhist.dat};
		\addplot table[x=x,y=5] {figs/avgalhist.dat};
\end{semilogyaxis}\end{tikzpicture}
\caption{Available galaxies (by kind) for a single fiber on a single tile}\label{avgalhist}
\end{minipage}%
\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{avtfhist}
\begin{tikzpicture}[scale=1.1]\begin{semilogyaxis}[const plot,enlarge x limits=false,xlabel={\# of TF that can access a given galaxy},xmax=13,ylabel={},cycle list name=mycolorlist,legend pos=south west]
		\addplot table[x=x,y=0] {figs/avtfhist.dat};
		\addplot table[x=x,y=1] {figs/avtfhist.dat};
		\addplot table[x=x,y=2] {figs/avtfhist.dat};
		\addplot table[x=x,y=3] {figs/avtfhist.dat};
		\addplot table[x=x,y=4] {figs/avtfhist.dat};
		\addplot table[x=x,y=5] {figs/avtfhist.dat};
\end{semilogyaxis}\end{tikzpicture}
\caption{Available tile-fibers for a galaxy (by kind)}\label{avtfhist}
\end{minipage}
\end{center}\end{figure}


\begin{figure}[H]\begin{center}
	\tikzsetnextfilename{reachplate}
\begin{tikzpicture}[scale=1.1]\begin{axis}[const plot,enlarge x limits=false,enlarge y limits=false,xlabel={\# of plates that can access a given galaxy},ylabel={\%},cycle list name=mycolorlist,legend entries={Pacman,Normal},legend pos=north east]
		\addplot table[x=x,y=0] {figs/reachplatepac.dat};
		\addplot table[x=x,y=0] {figs/reachplate.dat};
\end{axis}\end{tikzpicture}
\caption{Available plates for a galaxy (without 5th pass)}\label{reachplate}
\end{center}\end{figure}


\subsection{Results of the assignment}
We run the program with all information (prior knowledge of information on fake, target, etc) to compare with realistic simulation, this result is interesting.

The results are then written using {\tt display\_results}, and each assignment/improvement function display its own statistics.

We precise that we have indeed exactly 10 SS and 40 SF per petal. We have taken a reference strategy, which was a trade-off between computation time and quality of results. We use improvement functions in a way such that they are still efficient given the time they take (because the more improvement execution we launch, the less they are efficent). And from this reference strategy, we change parameters to see the effects of them on the program. We also compare to other strategies. 

The best strategy (discovered until now) is the one where we first look at 2000 tiles, tile after tile runing simple\_assign on each tile without plan, and we then do a plan for all left tiles, before improving it only with an SF-improve.

In the second plan, we can experimentally see that after a first SF-improvement ($+\sim 3.6\%$), even if we redistribute, a second one is almost useless ($+\sim 0.1\%$), whereas a second simple-improve is still efficient after a redistribution ($+\sim 0.4\%$), but useless without a redistribution step ($+\sim 0.01\%$). An other kind of kind-improvement (instead of SF and SS ($+\sim 0.5\%$)) is also almost useless. Furthermore, the computation time is far bigger for SF-improvement (5 mn) than for other.

The most results-sensible feature in it is the number of plan/applying and their plates sets. One could try to find a better one.


\subsection{Sum-up of assigned galaxies}
In the reference strategy, we have 50,095,513 assignments in total (93.54\% of all fibers). The weighted score of a certain kind is defined as : 
\vspace{1\baselineskip}
$Score(kind) = 100\cdot \frac{\sum\nolimits_{g \in kind} obs(g)}{\sum\nolimits_{g \in kind} goal}$ where $obs(g)$ is the number of times g is observed.

A sum-up table of general results on assignment is provided on Table \ref{res}.

\begin{comment}
\begin{table}[H]\begin{center}
\begin{tabular}{r|rrrrrrrcc}
	~   &         0 &         1 &       2 &      3 &      4 &     5 &         Total &  observed $\%$ & weighted $\%$ \\ \hline
	QSO \lya   &    22,915 &   414,720 & 238,413 & 75,600 & 28,497 & 8,498 &   788,643 & 98.922 & 67.670\\ 
	QSO Tracer & 1,874,187 &    20,433 &       0 &      0 &      0 &     0 & 1,894,620 & 98.921 & 98.921\\ 
	LRG        & 3,909,020 &   562,287 & 245,036 &      0 &      0 &     0 & 4,716,343 & 94.804 & 88.843\\ 
	ELG        &29,680,067 & 8,392,913 &       0 &      0 &      0 &     0 &38,072,980 & 77.955 & 77.955\\ 
	Fake QSO   & 1,405,670 &    15,342 &       0 &      0 &      0 &     0 & 1,421,012 & 98.920 & 98.920\\ 
	Fake LRG   &   753,871 &    35,579 &       0 &      0 &      0 &     0 &   789,450 & 95.493 & 95.493\\ 
	SS         & 1,066,600 & 1,143,862 &       0 &      0 &      0 &     0 & 2,210,462 & 48.252 & 48.252\\ 
	SF         & 4,266,400 &17,838,234 &       0 &      0 &      0 &     0 &22,104,634 & 19.300 & 19.300\\ 
\end{tabular}
\caption{Remaining observations (id on lines, nobs left on rows) with total (old version with collide = 2.1)}\label{gen}
\end{center}\end{table}
\end{comment}

\begin{table}[H]\begin{center}
\begin{tabular}{rrrrrrrrrrcc}
\hline
\multicolumn{6}{r}{Times observed} \\
	~ &           0 &     1 &  2 & 3 & 4 & 5 &  Total & Fiber used & Once observed & observed $\%$ & weighted $\%$ \\ \hline
   QSO Ly-a   &     0 &     2 &   4 & 14 & 25 & 1 &    49 &   167 &    49 & 98.434 & 67.032\\ 
 QSO Tracer   &     1 &   118 &   0 &  0 &  0 & 0 &   119 &   118 &   118 & 98.454 & 98.454\\ 
        LRG   &    16 &    37 & 244 &  0 &  0 & 0 &   298 &   526 &   281 & 94.376 & 88.069\\ 
        ELG   &   537 & 1,874 &   0 &  0 &  0 & 0 & 2,411 & 1,874 & 1,874 & 77.721 & 77.721\\ 
   Fake QSO   &     1 &    88 &   0 &  0 &  0 & 0 &    90 &    88 &    88 & 98.446 & 98.446\\ 
   Fake LRG   &     2 &    47 &   0 &  0 &  0 & 0 &    50 &    47 &    47 & 95.063 & 95.063\\ 
         SS   &    72 &    67 &   0 &  0 &  0 & 0 &   140 &    67 &    67 & 48.252 & 48.252\\ 
         SF   & 1,129 &   270 &   0 &  0 &  0 & 0 & 1,400 &   270 &   270 & 19.300 & 19.300\\ 
\hline
\end{tabular}
\caption{Densities (objects/\sqd) as as a function of \# of observations (with total), and \% observed, once and weighted}\label{res}
\end{center}\end{table}


\subsection{Pacman plate}
On the Table \ref{pac}, we present results of the assignment taking for input the fiber locations files of the pacman (plate deprived of 4 petals) used in case of run out of money. Every parameter is the same. As one can notice, they are quite similar to regular ones.

\begin{table}[H]\begin{center}
\begin{tabular}{rrrrrrrrrrcc}
\hline
\multicolumn{6}{r}{Times observed} \\
	~ &           0 &     1 &  2 & 3 & 4 & 5 &  Total & Fiber used & Once observed & observed $\%$ & weighted $\%$ \\ \hline
   QSO Ly-a   &     1 &     1 &   4 & 14 & 27 & 0 &    49 &   166 &    48 & 97.593 & 66.818 \\ 
 QSO Tracer   &     2 &   117 &   0 &  0 &  0 & 0 &   119 &   117 &   117 & 97.575 & 97.575 \\ 
        LRG   &    17 &    33 & 247 &  0 &  0 & 0 &   298 &   528 &   280 & 93.987 & 88.411 \\ 
        ELG   &   493 & 1,918 &   0 &  0 &  0 & 0 & 2,411 & 1,918 & 1,918 & 79.540 & 79.540 \\ 
   Fake QSO   &     2 &    87 &   0 &  0 &  0 & 0 &    90 &    87 &    87 & 97.579 & 97.579 \\ 
   Fake LRG   &     2 &    47 &   0 &  0 &  0 & 0 &    50 &    47 &    47 & 94.608 & 94.608 \\ 
         SS   &    70 &    69 &   0 &  0 &  0 & 0 &   140 &    69 &    69 & 49.576 & 49.576 \\ 
         SF   & 1,122 &   277 &   0 &  0 &  0 & 0 & 1,400 &   277 &   277 & 19.830 & 19.830 \\ 
\hline
\end{tabular}
\caption{Same than Table \ref{res} but with the pacman}\label{pac}
\end{center}\end{table}

Furthermore, there are 50,938,132 assignments in total (92.9358 \% of all fibers).

\subsection{Free fibers}
Here are the histogram of petals as a function of free fibers on Figure \ref{freefib} and the number of free fibers for each plate, in inscreasing order on Figure \ref{fft}.


\begin{figure}[H]\begin{center}\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{freefib}
	\begin{tikzpicture}[scale=1.1]\begin{semilogyaxis}[const plot,enlarge x limits=false,xlabel={\# of free fibers},ylabel={\# of petals},xmax=160] 
	\addplot [mark=none] table [x=x,y=0] {figs/freefib.dat};
\end{semilogyaxis}\end{tikzpicture}
\caption{\# of petals with that many free fibers}\label{freefib}
 \end{minipage}%
\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{fft}
	\begin{tikzpicture}[scale=1.1]
		\begin{axis}[const plot,enlarge x limits=false,enlarge y limits=false,xlabel={Tiles observed},ylabel={\# of free fibers},cycle list name=mycolorlist]
			\addplot table[x=x,y=0] {figs/fft.dat};
		\end{axis}
	\end{tikzpicture}
	\caption{Free fibers as a function of time (plates)}\label{fft}
 \end{minipage}
\end{center}\end{figure}


On the Figure \ref{seendens}, one can seen the proportion of observed objects as a function of their density. "usq" stands for unity of square degrees, its the area of the sky reachable by a single fiber. Keep Figure \ref{avgalhist} in mind while reading this one.
\begin{figure}[H]\begin{center}\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{seendens}
\begin{tikzpicture}[scale=1.1]
	\begin{axis}[enlarge y limits=false,enlarge x limits=false,xlabel={Objects/usq},ylabel={},cycle list name=mycolorlist,legend entries={QSO \lya,QSO Tracer,LRG,ELG,Fake QSO,Fake LRG,All},legend pos=south east]
		\addplot table[x=x,y=0] {figs/seendens.dat};
		\addplot table[x=x,y=1] {figs/seendens.dat};
		\addplot table[x=x,y=2] {figs/seendens.dat};
		\addplot table[x=x,y=3] {figs/seendens.dat};
		\addplot table[x=x,y=4] {figs/seendens.dat};
		\addplot table[x=x,y=5] {figs/seendens.dat};
		\addplot table[x=x,y=8] {figs/seendens.dat};
	\end{axis}
\end{tikzpicture}
\end{minipage}%
\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{seendenszoom}
\begin{tikzpicture}[scale=1.1]
	\begin{axis}[xmax=7,ymin=65,enlarge y limits=false,enlarge x limits=false,xlabel={Objects/usq},ylabel={},cycle list name=mycolorlist]
		\addplot table[x=x,y=0] {figs/seendens.dat};
		\addplot table[x=x,y=1] {figs/seendens.dat};
		\addplot table[x=x,y=2] {figs/seendens.dat};
		\addplot table[x=x,y=3] {figs/seendens.dat};
		\addplot table[x=x,y=4] {figs/seendens.dat};
		\addplot table[x=x,y=5] {figs/seendens.dat};
		\addplot table[x=x,y=8] {figs/seendens.dat};
	\end{axis}
\end{tikzpicture}
 \end{minipage}
\caption{\% of observed galaxies as a function of objects density, wide and zoom}\label{seendens}
\end{center}\end{figure}


\subsection{Redistribution-Improvement step}
We see, in the second plan, what is the effect of several redistribution-improvement executions. Here is presented, in the Table \ref{ris}, the number of redistributions, number of additionnal assignments, with the improvement in \% and the total time taken for until this step.

\begin{table}[H]\centering
	\begin{tabular}{c|cccl}
		Step & \# red & \# +as & + \% & Time \\ \hline
		1 & 2,204,977 & 167,741 & .418 & 40s\\
		2 & 2,060,851 & 76,509 & .190 & 1mn 20s\\
		3 & 1,965,994 & 19,273 & 0.048 & 2mn\\
		4 & 1,945,418 & 10,738 & 0.027 & 2mn 40s\\
		5 & 1,929,917 & 7,999 & 0.020 & 3mn 20s\\
		6 & 1,921,331 & 5,745 & 0.014 & 4mn\\
	\end{tabular}
	\caption{Results of several redistribution-improvement steps}\label{ris}
\end{table}


\subsection{Results on \lya particularly}
The figure \ref{histastf} gives the histogram of proportion of assigned \lya and the number of observations as a function of available TF.

\begin{figure}[H]\begin{center}
	\tikzsetnextfilename{obsly}
\begin{tikzpicture}[scale=1.1]
	\begin{axis}[const plot,stack plots=y,enlarge x limits=false,enlarge y limits=false,xlabel={Number of available TF},ylabel={},cycle list name=mycolorlist,legend entries={0,1,2,3,4,5}]
		\addplot [fill,fill opacity=0.5,color=blue] table[x=x,y=0] {figs/obsly.dat};
		\addplot [fill,fill opacity=0.5,color=red] table[x=x,y=1] {figs/obsly.dat};
		\addplot [fill,fill opacity=0.5,color=brown] table[x=x,y=2] {figs/obsly.dat};
		\addplot [fill,fill opacity=0.5,color=teal] table[x=x,y=3] {figs/obsly.dat};
		\addplot [fill,fill opacity=0.5,color=violet] table[x=x,y=4] {figs/obsly.dat};
		\addplot [fill,fill opacity=0.5,color=black] table[x=x,y=5] {figs/obsly.dat};
	\end{axis}
\end{tikzpicture}
\caption{\# of QSO \lya (with their number of observation) as a function of available tile-fibers}\label{histastf}
\end{center}\end{figure}


\begin{figure}[H]\begin{center}\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{dist2ly}
\begin{tikzpicture}[scale=1.1]
		\begin{semilogyaxis}[const plot,enlarge x limits=false,xlabel={Distance between plates},ylabel={}]
			\addplot [color=blue,mark=none] table[x=x,y=0] {figs/dist2ly.dat};
		\end{semilogyaxis}
	\end{tikzpicture}
\end{minipage}%
\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{dist2lyint}
	\begin{tikzpicture}[scale=1.1]
		\begin{axis}[const plot,enlarge x limits=false,enlarge y limits=false,xlabel={Distance between plates},ylabel={}]
			\addplot [mark=none] table[x=x,y=1] {figs/dist2ly.dat};
		\end{axis}
	\end{tikzpicture}
 \end{minipage}
	\caption{Histogram of distances (in number of plates) between two consecutive observed QSO \lya and integral}
\end{center}\end{figure}


\subsection{Study on evolution over time}

\begin{figure}[H]\begin{center}
	\tikzsetnextfilename{time2}
\begin{tikzpicture}[scale=1.5]
	\begin{semilogyaxis}[enlarge y limits=false,enlarge x limits=false,xlabel={Tiles observed},ylabel={\# of objects},legend entries={QSO \lya 1 obs,QSO \lya 2 obs,QSO \lya 3 obs,QSO \lya 4 obs,QSO \lya 5 obs,LRG 1 obs,LRG 2 obs,QSO Tracer,ELG},legend style={at={(1.3,0.9)}, anchor=north}]
		\addplot [color=red,loosely dashed] table[x=x,y=0] {figs/time2.dat};
		\addplot [color=red,dashed] table[x=x,y=1] {figs/time2.dat};
		\addplot [color=red,dashdotted] table[x=x,y=2] {figs/time2.dat};
		\addplot [color=red,densely dashdotted] table[x=x,y=3] {figs/time2.dat};
		\addplot [color=red] table[x=x,y=4] {figs/time2.dat};
		\addplot [color=blue,densely dashed] table[x=x,y=5] {figs/time2.dat};
		\addplot [color=blue] table[x=x,y=6] {figs/time2.dat};
		\addplot [color=green] table[x=x,y=7] {figs/time2.dat};
		\addplot [color=cyan] table[x=x,y=8] {figs/time2.dat};
	\end{semilogyaxis}
\end{tikzpicture}
\caption{Observed galaxy kind as a function of time (plates seen)}
\end{center}\end{figure}


\subsection{Shifting parameters}
In the following, we change a little bit parameters around the references values to observe their influences on results.

\begin{figure}[H]\begin{center}\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{colllya}
	\begin{tikzpicture}[scale=1.1]
		\begin{axis}[enlarge x limits=false,xlabel={Collide (mm)},ylabel={Weighted \% of \lya},cycle list name=mycolorlist]
			\addplot [mark=x] coordinates {(0,68.572)(1,68.414)(1.8,67.963)(2.0,67.774)(2.1,67.670)(2.2,67.545)(2.4,67.278)(2.8,66.606)(3,66.187)(3.2,65.741)};
			\draw [densely dashed] (axis cs:2.1,\pgfkeysvalueof{/pgfplots/ymin}) -- (axis cs:2.1,\pgfkeysvalueof{/pgfplots/ymax});
		\end{axis}
	\end{tikzpicture}
\end{minipage}%
\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{collelg}
	\begin{tikzpicture}[scale=1.1]
		\begin{axis}[enlarge x limits=false,xlabel={Collide (mm)},ylabel={\% of ELG},cycle list name=mycolorlist]
			\addplot [mark=x] coordinates {(0,78.301)(1,78.073)(1.8,77.999)(2.0,77.970)(2.1,77.955)(2.2,77.937)(2.4,77.894)(2.8,77.777)(3,77.703)(3.2,77.612)};
			\draw [densely dashed] (axis cs:2.1,\pgfkeysvalueof{/pgfplots/ymin}) -- (axis cs:2.1,\pgfkeysvalueof{/pgfplots/ymax});
		\end{axis}
	\end{tikzpicture}
\end{minipage}
	\caption{Weighted \% of \lya and \% of ELG observed as a function of required stay-clear}
\end{center}\end{figure}


\begin{figure}[H]\begin{center}
	\tikzsetnextfilename{intlya}
	\begin{tikzpicture}[scale=1.1]
		\begin{axis}[enlarge x limits=false,xlabel={InterPlate},ylabel={Weighted \% of \lya},cycle list name=mycolorlist]
			\addplot [mark=x] coordinates {(0,73.317)(1,70.172)(20,68.022)(50,67.961)(100,67.905)(200,67.670)(400,67.461)(1000,66.888)(2000,65.660)};
			\draw [densely dashed] (axis cs:200,\pgfkeysvalueof{/pgfplots/ymin}) -- (axis cs:200,\pgfkeysvalueof{/pgfplots/ymax});
		\end{axis}
	\end{tikzpicture}
	\caption{\% of \lya observed as a function of required minimum interval between plates observing the \lya}
\end{center}\end{figure}


\begin{comment}
\end{comment}


\section{Possible improvements}
We havn't tried all possible strategies, so it is possible that an other combination of runing of the functions could lead to better results.
One could also try to search for an "orthogonal" (of the 3 other ones) improvement function.
To make plans, we could use an automaton, which would be automatically able to use all kinds of assigment/improvement functions, and would go through all fibers at random to assign/improve randomly, and the controlable parameter would only be the time we want it to work.
Nevertheless, a lot of efforts now lead to little improvements of the results. It is likely that we are almost at the global optimum. We only have a doubt on the number of plans/applying and their sets of plates that could improve significantly.

The problem of colliding fiber positioners is addressed in a simplified way (circles around a fiber) which can be precised.

We can adapt a little bit the code in order to do assignment and improvements without SS and SF, and only add SS and SF just before launching the observation, using the "replace" function in global.cpp. It's not likely to improve a lot because improve-SS and SF must already improve this way.

The update function might still be improved to lead to more reassignments, and an improvement execution on the current plan could be efficient after several hundred updatings, since some "redistribution" happened with updatings.

We could make plans with only \lya assigned, launching improve function, and then adding separately LRG then improving, and adding again ELG and improving. It's likely that it won't increase results because as we saw before, doing assignments separately in new\_assign already didn't improved results. This kind of optimization must actually be already done by selecting with priorities and executing improvement functions.

\section{Collision problem}
\begin{figure}[H]\begin{center}
	\tikzsetnextfilename{col}
\begin{tikzpicture}[scale=1.1]
	\begin{axis}[enlarge y limits=false,enlarge x limits=false,xlabel={Distance between fibers (mm)},ylabel={\# of collision cases}]
		\addplot [mark=none] table[x=x,y=0] {figs/col.dat};
	\end{axis}
\end{tikzpicture}
\caption{Histogram of cases of collision}
\end{center}\end{figure}

We will forbid distance between two galaxies to be less than 2 times the radius of what we will further call the first disk (FD), which is the disk of radius 0.990 mm that incorporates the hole for fiber ferrule. 
It is sure that it is a necessary condition (simple to convince oneself) and will avoid a good proportion of collisions (40\% ?) using a very cheap computation.
There is also 3-collisions problem possible, even with this constrain.
4-collision problem is possible without this constrain, but maybe impossible with the constrain.


At the moment of the study, we need to have a good precision of the final assignment efficacity. Joe Silber wrote a collision code with the exact geometry of positionners, and found out that the collision rate is 8\%. To take it into account, we calibrate the $collide$ parameter to get this same collision rate. Of course, this is not exact and eventually needs to be so, but by this way we get results very close to the final ones. We experimentally found out that we have to take $collide = 2.9 mm$.
Notice that with collisions allowed, we get 94.4 \% of fibers assigned, instead of 93.9 \% when they are not. A further improvement can be to do the assignment with collisions allowed and at the very end try to change the colliding ones, but it's likely to lead to the same results than now.


\begin{table}[H]\centering
	\begin{tabular}{r|ccc}
		Collide (mm) & 2.1 & 3.0 & 2.9\\ 
		Collision rate at 2000 (\%) & 3.58&8.44 & 7.75\\
		Collision rate (\%) & 3.93 & ~ & 8.27\\
	\end{tabular}
	\caption{Collision rate as a function of Collide}\label{colrate}
\end{table}

Note here that the collision rate is less when we only take the first 2000 plates, though on those plates more fibers are assigned, so one should get a greater percentage than when we take all the plates. Still to understand.


\begin{table}[H]\begin{center}
\begin{tabular}{rrrrrrrrrrcc}
\hline
\multicolumn{6}{r}{Times observed} \\
	~ &           0 &     1 &  2 & 3 & 4 & 5 &  Total & Fiber used & Once observed & observed $\%$ & weighted $\%$ \\ \hline
   QSO Ly-a   &     0 &     2 &   4 & 14 & 25 & 1 &    49 &   167 &    49 & 98.434 & 67.032\\ 
 QSO Tracer   &     1 &   118 &   0 &  0 &  0 & 0 &   119 &   118 &   118 & 98.454 & 98.454\\ 
        LRG   &    16 &    37 & 244 &  0 &  0 & 0 &   298 &   526 &   281 & 94.376 & 88.069\\ 
        ELG   &   537 & 1,874 &   0 &  0 &  0 & 0 & 2,411 & 1,874 & 1,874 & 77.721 & 77.721\\ 
   Fake QSO   &     1 &    88 &   0 &  0 &  0 & 0 &    90 &    88 &    88 & 98.446 & 98.446\\ 
   Fake LRG   &     2 &    47 &   0 &  0 &  0 & 0 &    50 &    47 &    47 & 95.063 & 95.063\\ 
         SS   &    72 &    67 &   0 &  0 &  0 & 0 &   140 &    67 &    67 & 48.252 & 48.252\\ 
         SF   & 1,129 &   270 &   0 &  0 &  0 & 0 & 1,400 &   270 &   270 & 19.300 & 19.300\\ 
\hline
\end{tabular}
\caption{Results for collide = 2.9}\label{29}
\end{center}\end{table}
93.54 \% assigned of all fibers.



\begin{table}[H]\begin{center}
\begin{tabular}{rrrrrrrrrrcc}
\hline
\multicolumn{6}{r}{Times observed} \\
	~ &           0 &     1 &  2 & 3 & 4 & 5 &  Total & Fiber used & Once observed & observed $\%$ & weighted $\%$ \\ \hline
   QSO Ly-a   &     0 &     1 &   4 & 13 & 27 & 1 &    49 &   170 &    49 & 98.611 & 68.228\\ 
 QSO Tracer   &     1 &   118 &   0 &  0 &  0 & 0 &   119 &   118 &   118 & 98.626 & 98.626\\ 
        LRG   &    15 &    35 & 247 &  0 &  0 & 0 &   298 &   530 &   283 & 94.794 & 88.823\\ 
        ELG   &   532 & 1,879 &   0 &  0 &  0 & 0 & 2,411 & 1,879 & 1,879 & 77.935 & 77.935\\ 
   Fake QSO   &     1 &    88 &   0 &  0 &  0 & 0 &    90 &    88 &    88 & 98.617 & 98.617\\ 
   Fake LRG   &     2 &    47 &   0 &  0 &  0 & 0 &    50 &    47 &    47 & 95.443 & 95.443\\ 
         SS   &    72 &    67 &   0 &  0 &  0 & 0 &   140 &    67 &    67 & 48.252 & 48.252\\ 
         SF   & 1,129 &   270 &   0 &  0 &  0 & 0 & 1,400 &   270 &   270 & 19.300 & 19.300\\ 
\hline
\end{tabular}
\caption{Results for collide = 2.1}\label{21}
\end{center}\end{table}
93.93 \% assigned of all fibers.

\begin{figure}[H]\begin{center}\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{coldist}
\begin{tikzpicture}[scale=1.1]
		\begin{axis}[const plot,enlarge x limits=false,enlarge y limits=false,xlabel={Distances between two colliding positioners},ylabel={}]
			\addplot [color=blue,mark=none] table[x=x,y=0] {figs/coldist.dat};
		\end{axis}
	\end{tikzpicture}
\end{minipage}%
\begin{minipage}{.5\textwidth}
	\tikzsetnextfilename{coldistint}
	\begin{tikzpicture}[scale=1.1]
		\begin{axis}[const plot,enlarge x limits=false,enlarge y limits=false,xlabel={Distances between two colliding positioners},ylabel={}]
			\addplot [mark=none] table[x=x,y=1] {figs/coldist.dat};
		\end{axis}
	\end{tikzpicture}
 \end{minipage}
	\caption{Histogram of distances between two colliding positioners}
\end{center}\end{figure}

\begin{figure}[H]
\hspace*{-9.2cm}
    \includegraphics[scale=1.75]{python/tile.pdf}
	\caption{Tile}
\end{figure}

Polygons consist in segments and circles. Modelizing an arc by a circle is much better than by segments, because it needs only one circle, instead of several segments. Computationally cheap methods of collision checking between segments or circles can be read in collision.cpp.

\bibliographystyle{plain}
\bibliography{papers}{}

\end{document}
